# hkb & HNTW
# __author__="harpreet kaur bargota" & "Hao Nan Tobey Wang"
# __email__="harpreet.bargota@agr.gc.ca" & "haonantobey.wang@agr.gc.ca"
# __Project__="Faba bean Feature extraction pipeline (Step2)"
# __Date__=2025/11/24

#References: 
#SAM 2: Segment Anything in Images and Videos:https://github.com/facebookresearch/sam2
#Reference paper: https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/

#Feature extraction: 
#scikit-image library for image processing: Stéfan van der Walt, Johannes L. Schönberger, Juan Nunez-Iglesias, François Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu and the scikit-image contributors. scikit-image: Image processing in Python. PeerJ 2:e453 (2014) https://doi.org/10.7717/peerj.453
#https://scikit-image.org/docs/stable/api/skimage.measure.html

"""
This script processes the Segment Anything (SAM) masks for Faba bean images and extracts
dimensional and shape features. Three types of calibration are performed for pixel-to-mm
conversion using a coin standard in each image:

1. Original SAM calibration
2. Taubin SVD-based circle calibration
3. OpenCV minEnclosingCircle calibration

The output CSV contains all the calculated features for comparison.
"""

import argparse
import pandas as pd
import numpy as np
import cv2
import os
import glob
import warnings
warnings.filterwarnings(action='ignore')
from skimage import measure
from skimage.measure import regionprops_table
from circle_fit import taubinSVD

def classify_shape(row):
    """
    Identifies the shape of beans based on the shapefactors.
    
    Parameters:
    row : pandas.Series
        A row from the dataframe containing shapefactors.
    
    Returns:
    str
        A comma-separated string indicating the shape classification.
    """
    shape1 = 'Elongated' if row['Shapefactor1'] <= 0.5 else 'Compact'
    shape2 = 'Oval' if row['Shapefactor2'] <= 0.5 else 'Circular'
    shape3 = 'Circular' if row['Shapefactor3'] >= 0.9 else 'Elongated'
    shape4 = 'Ellipse' if row['Shapefactor4'] >= 0.9 else 'Irregular'
    return f"{shape1},{shape2},{shape3},{shape4}"

def process_SAMmasks(SAM_masks, output_folder):      
    """
    Calculates the features from metadata files and binary masks generated by Segmentanything model SAM 2.1. It analyzes the data, filters the bean masks,
    checks the circularity of beans, creates annotated combined binary mask for each image, calculates the seed count and extracts the 
    features of beans in pixels using image processing libraries. It also standardizes the features from pixels to metric units and saves 
    the annotated binary masks, features .csv files and seed count .xlsx files in the output folder. 

    Arguments:
    SAM_masks: The folder containing the subfolders for each image with binary masks and metadata.Expects the
    binary masks in .png format, with pixel values in [0, 255] and a metadata file in .csv format generated by Segmentanything model SAM.
    output_folder: The output folder where the annotated binary masks, feature extraction .csv file and seed count .xlxs will be saved.

    Raises:
    TypeError: If the condiotions are not met.
    """   
    # Ensure paths exist
    if not os.path.exists(SAM_masks):
        print(f"Error: SegmentAnything masks folder '{SAM_masks}' does not exist.")
        return
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
        print(f"Output folder '{output_folder}' created.")

    df_total = []

    # Loop through each subfolder (each image)
    for subfolder_name in os.listdir(SAM_masks):
        subfolder_path = os.path.join(SAM_masks, subfolder_name)
        if not os.path.isdir(subfolder_path):
            continue
        print(f'Opening subfolder: {subfolder_path}')

        # Load metadata CSV for the image
        csv_file = glob.glob(os.path.join(subfolder_path, '*.csv'))
        df_metadata = pd.read_csv(csv_file[0])

        # Identify coin mask(s) for standardization
        df_metadata_coin = df_metadata[(df_metadata['bbox_x0'] >= 3000) & (df_metadata['area'] >= 200000)]
        Mask_index1 = df_metadata_coin.index.tolist()
        Mask_index = Mask_index1[:1]  # only first coin
        print('Index of coin mask is', Mask_index)

        # Remove non-bean masks based on coordinates, size, or area
        conditions = [
            (df_metadata['bbox_x0'] <= 2800) & (df_metadata['bbox_y0'] <= 1950),  # Colorcard
            (df_metadata['bbox_x0'] <= 1900) & (df_metadata['bbox_y0'] >= 4650),  # Label
            (df_metadata['bbox_x0'] <= 4000) & (df_metadata['bbox_y0'] >= 5200),  # Scale
            (df_metadata['bbox_x0'] >= 3000) & (df_metadata['area'] >= 200000),   # Coin
            (df_metadata['bbox_x0'] >= 3000) & (df_metadata['bbox_y0'] >= 4400),  # Coin 
            (df_metadata['area'] <= 5000),   # Tiny masks & un-specific areas
            (df_metadata['bbox_h'] >= 1900),  # duplicate masks 
            (df_metadata['bbox_w'] >= 700)  # duplicate masks 
        ]
        for cond in conditions:
            df_metadata = df_metadata.drop(df_metadata[cond].index)

        mask_filenames = df_metadata.index.to_list()
        print("Bean masks after removing non-specific masks:", mask_filenames)

        combined_mask = None
        circularity_threshold = 0.7
        df_list = []

        # Load coin mask for calibration using regionprops
        coin_file_path = os.path.join(subfolder_path, f'{Mask_index[0]}.png')
        mask_coin = cv2.imread(coin_file_path, cv2.IMREAD_GRAYSCALE)
        label_coin = measure.label(mask_coin)
        props_coin = regionprops_table(label_coin, properties=('area','perimeter','axis_major_length','axis_minor_length'))
        df_coin = pd.DataFrame(props_coin).iloc[0]

        # Calibration constants
        Length_coin_mm = 23.88
        width_coin_mm = 23.88

        # Original SAM calibration factors
        Area_standard_coin_pixels = df_coin['area']
        Area_standard_coin_mm2 = np.pi * (Length_coin_mm/2)**2
        Calibration_factor_area = Area_standard_coin_mm2 / Area_standard_coin_pixels

        axis_major_length_pixels = df_coin['axis_major_length']
        Calibration_factor_length = Length_coin_mm / axis_major_length_pixels

        axis_minor_length_pixels = df_coin['axis_minor_length']
        Calibration_factor_width = width_coin_mm / axis_minor_length_pixels

        perimeter_mm = (2 * np.pi * Length_coin_mm) / 2
        perimeter_pixels = df_coin['perimeter']
        Calibration_factor_perimeter = perimeter_mm / perimeter_pixels

        # --- Process each bean mask ---
        for mask_filename in mask_filenames:
            file_path = os.path.join(subfolder_path, f'{mask_filename}.png')
            mask = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)
            if mask is None:
                print(f"Warning: {file_path} not found or couldn't be loaded.")
                continue

            # Filter masks based on circularity
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            include_mask = False
            circularity_values = []
            for contour in contours:
                area = cv2.contourArea(contour)
                perimeter = cv2.arcLength(contour, True)
                if perimeter > 0:
                    circularity = (4 * np.pi * area) / (perimeter ** 2)
                    circularity_values.append(circularity)
                    if circularity > circularity_threshold and all(x >= 0.5 for x in circularity_values):
                        include_mask = True
                        break

            if include_mask:
                if combined_mask is None:
                    combined_mask = mask.copy()
                else:
                    combined_mask = cv2.bitwise_or(combined_mask, mask)

        if combined_mask is None:
            continue

        # Annotate combined mask
        _, combined_mask = cv2.threshold(combined_mask, 127, 255, cv2.THRESH_BINARY)
        contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        contour_image = cv2.cvtColor(combined_mask, cv2.COLOR_GRAY2BGR)
        cv2.drawContours(contour_image, contours, -1, (0, 0, 255), 15)
        cv2.imwrite(os.path.join(output_folder, f"{subfolder_name}_combined_mask.png"), contour_image)

        # Extract features using regionprops
        label_image = measure.label(combined_mask)
        props = regionprops_table(label_image, properties=('centroid','bbox','area','eccentricity', 
                                                           'equivalent_diameter_area','perimeter','solidity', 
                                                           'area_convex','extent','axis_major_length','axis_minor_length'))
        df_FE = pd.DataFrame(props)

        # Shape features
        df_FE = df_FE.assign(
            Aspect_Ratio=lambda x: x["axis_major_length"] / x["axis_minor_length"],
            Roundness=lambda x: (4 * np.pi * x["area"]) / (x["perimeter"] ** 2),
            Compactness=lambda x: x["equivalent_diameter_area"] / x["axis_major_length"]
        )
        df_FE["Circularity-SAM"] = 1 / df_FE["Roundness"]
        df_FE["Shapefactor1"] = df_FE["axis_major_length"] / df_FE["area"]
        df_FE["Shapefactor2"] = df_FE["axis_minor_length"] / df_FE["area"]
        df_FE["Shapefactor3"] = df_FE["area"] / ((df_FE["axis_major_length"] / 2) ** 2 * np.pi)
        df_FE["Shapefactor4"] = df_FE["area"] / ((df_FE["axis_major_length"] / 2) * (df_FE["axis_minor_length"] / 2) * np.pi)
        df_FE["class"] = subfolder_name.split('.JPG')[-1]

        # --- Calibrated measurements ---
        # Original SAM
        df_FE["Area_mm2_SAM"] = df_FE["area"] * Calibration_factor_area
        df_FE["Length_mm_SAM"] = df_FE["axis_major_length"] * Calibration_factor_length
        df_FE["Width_mm_SAM"] = df_FE["axis_minor_length"] * Calibration_factor_width
        df_FE["Perimeter_mm_SAM"] = df_FE["perimeter"] * Calibration_factor_perimeter

        # Taubin SVD
        contours_coin, _ = cv2.findContours(mask_coin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
        cnt = contours_coin[0]
        pts = np.vstack(cnt).squeeze()
        xc, yc, radius_pixels, sigma = taubinSVD(pts)
        Area_standard_coin_pixels_taubin = np.pi * radius_pixels**2
        Calibration_factor_area_taubin = Area_standard_coin_mm2 / Area_standard_coin_pixels_taubin
        Calibration_factor_length_taubin = Length_coin_mm / (2 * radius_pixels)
        Calibration_factor_width_taubin = width_coin_mm / (2 * radius_pixels)
        Calibration_factor_perimeter_taubin = perimeter_mm / (2 * np.pi * radius_pixels)
        df_FE["Area_mm2_SAM_taubin"] = df_FE["area"] * Calibration_factor_area_taubin
        df_FE["Length_mm_SAM_taubin"] = df_FE["axis_major_length"] * Calibration_factor_length_taubin
        df_FE["Width_mm_SAM_taubin"] = df_FE["axis_minor_length"] * Calibration_factor_width_taubin
        df_FE["Perimeter_mm_SAM_taubin"] = df_FE["perimeter"] * Calibration_factor_perimeter_taubin

        # Min Enclosing Circle
        (x_center, y_center), radius = cv2.minEnclosingCircle(cnt)
        Area_standard_coin_pixels_min = np.pi * radius**2
        Calibration_factor_area_min = Area_standard_coin_mm2 / Area_standard_coin_pixels_min
        Calibration_factor_length_min = Length_coin_mm / (2 * radius)
        Calibration_factor_width_min = width_coin_mm / (2 * radius)
        Calibration_factor_perimeter_min = perimeter_mm / (2 * np.pi * radius)
        df_FE["Area_mm2_SAM_minEnc"] = df_FE["area"] * Calibration_factor_area_min
        df_FE["Length_mm_SAM_minEnc"] = df_FE["axis_major_length"] * Calibration_factor_length_min
        df_FE["Width_mm_SAM_minEnc"] = df_FE["axis_minor_length"] * Calibration_factor_width_min
        df_FE["Perimeter_mm_SAM_minEnc"] = df_FE["perimeter"] * Calibration_factor_perimeter_min

        df_list.append(df_FE)
        df_FE2 = pd.concat(df_list)
        df_total.append(df_FE2)

    # Combine all subfolders
    df_image = pd.concat(df_total)
    df_image['Shape'] = df_image.apply(classify_shape, axis=1)

    # Reorder columns
    df_image = df_image.loc[:, [
        'class',
        'Area_mm2_SAM','Length_mm_SAM','Width_mm_SAM','Perimeter_mm_SAM',
        'Area_mm2_SAM_taubin','Length_mm_SAM_taubin','Width_mm_SAM_taubin','Perimeter_mm_SAM_taubin',
        'Area_mm2_SAM_minEnc','Length_mm_SAM_minEnc','Width_mm_SAM_minEnc','Perimeter_mm_SAM_minEnc',
        'centroid-0','centroid-1',
        'bbox-0','bbox-1','bbox-2','bbox-3',
        'area','eccentricity','equivalent_diameter_area','perimeter',
        'solidity','area_convex','extent','axis_major_length','axis_minor_length',
        'Aspect_Ratio','Roundness','Compactness','Circularity-SAM',
        'Shape','Shapefactor1','Shapefactor2','Shapefactor3','Shapefactor4'
    ]]

    df_image.rename(columns={
        'class': 'Class',
        'Area_mm2_SAM':'Area-SAM(mm2)',
        'Length_mm_SAM':'Length-SAM(mm)',
        'Width_mm_SAM':'Width-SAM(mm)',
        'Perimeter_mm_SAM':'Perimeter-SAM(mm)',
        'Area_mm2_SAM_taubin':'Area-SAM_taubin(mm2)',
        'Length_mm_SAM_taubin':'Length-SAM_taubin(mm)',
        'Width_mm_SAM_taubin':'Width-SAM_taubin(mm)',
        'Perimeter_mm_SAM_taubin':'Perimeter-SAM_taubin(mm)',
        'Area_mm2_SAM_minEnc':'Area-SAM_minEnc(mm2)',
        'Length_mm_SAM_minEnc':'Length-SAM_minEnc(mm)',
        'Width_mm_SAM_minEnc':'Width-SAM_minEnc(mm)',
        'Perimeter_mm_SAM_minEnc':'Perimeter-SAM_minEnc(mm)',
        'area':'Area-SAM(pix)',
        'eccentricity':'Eccentricity',
        'equivalent_diameter_area':'Equivalent diameter area',
        'perimeter':'Perimeter(pix)',
        'axis_major_length':'Axis Major Length-SAM(pix)',
        'axis_minor_length':'Axis Minor Length-SAM(pix)',
        'Aspect_Ratio':'Aspect Ratio'
    }, inplace=True)

    df_image.index.names = ['Seed No. per image']
    print (df_image)
    count_seed=df_image.value_counts("Class").to_frame()
    count=count_seed.rename(columns={'Class':'Class_ID', 'count':'Seed Count'})
    

#Save the final file of feature extraction from all images into output folder
    output_filename = f"Faba_bean_Features_extraction.csv"
    output_path= os.path.join(output_folder, output_filename)
    df_image.to_csv(output_path)
    print ("Dimensional and Shape feature extraction from faba bean images is completed.")

#Save the Seed Count data from all images into output folder
    
    output_filename = f"Seed Count.xlsx"
    output_path= os.path.join(output_folder, output_filename)
    count.to_excel(output_path)
    print ("Seed count from faba bean images is completed.")
      
if __name__ == "__main__":
    # Create ArgumentParser object
    parser = argparse.ArgumentParser(description="Process files from input folder to output folder.")

    # Add arguments for input and output folders
    parser.add_argument("SAM_masks", help="Path to the SAM masks")
    parser.add_argument("output_folder", help="Path to the output folder")

    # Parse the command-line arguments
    args = parser.parse_args()

    # Call the process_folders function with provided input and output folders
    process_SAMmasks(args.SAM_masks, args.output_folder)
